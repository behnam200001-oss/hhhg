#include "memory_manager.h"
#include <iostream>
#include <mutex>
#include <algorithm>  // Ø¨Ø±Ø§ÛŒ std::remove

MemoryManager& MemoryManager::getInstance() {
    static MemoryManager instance;
    return instance;
}

MemoryManager::~MemoryManager() {
    clearCache();
}

void* MemoryManager::allocate(size_t size, const std::string& tag) {
    void* ptr = nullptr;
    cudaError_t err = cudaMalloc(&ptr, size);
    if (err != cudaSuccess) {
        std::cerr << "âŒ Failed to allocate " << size << " bytes: " 
                  << cudaGetErrorString(err) << std::endl;
        return nullptr;
    }
    
    std::lock_guard<std::mutex> lock(pool_mutex);
    memory_pool[tag].push_back(ptr);
    
    return ptr;
}

void MemoryManager::deallocate(void* ptr, const std::string& tag) {
    if (!ptr) return;
    
    cudaError_t err = cudaFree(ptr);
    if (err != cudaSuccess) {
        std::cerr << "âŒ Failed to deallocate memory: " 
                  << cudaGetErrorString(err) << std::endl;
    }
    
    std::lock_guard<std::mutex> lock(pool_mutex);
    auto& pool = memory_pool[tag];
    auto it = std::remove(pool.begin(), pool.end(), ptr);
    pool.erase(it, pool.end());
}

void* MemoryManager::allocateUnified(size_t size, const std::string& tag) {
    void* ptr = nullptr;
    cudaError_t err = cudaMallocManaged(&ptr, size);
    if (err != cudaSuccess) {
        std::cerr << "âŒ Failed to allocate unified memory: " 
                  << cudaGetErrorString(err) << std::endl;
        return nullptr;
    }
    
    std::lock_guard<std::mutex> lock(pool_mutex);
    memory_pool[tag].push_back(ptr);
    
    return ptr;
}

cudaError_t MemoryManager::copyToDevice(void* dst, const void* src, size_t size, cudaStream_t stream) {
    return cudaMemcpyAsync(dst, src, size, cudaMemcpyHostToDevice, stream);
}

cudaError_t MemoryManager::copyToHost(void* dst, const void* src, size_t size, cudaStream_t stream) {
    return cudaMemcpyAsync(dst, src, size, cudaMemcpyDeviceToHost, stream);
}

void* MemoryManager::getPreallocated(size_t size, const std::string& tag) {
    std::lock_guard<std::mutex> lock(pool_mutex);
    auto& pool = memory_pool[tag];
    
    // Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø­Ø§ÙØ¸Ù‡ Ø¨Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…Ù†Ø§Ø³Ø¨
    for (auto it = pool.begin(); it != pool.end(); ++it) {
        // Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø­Ø§ÙØ¸Ù‡ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ…
        // Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ú¯ÛŒØŒ Ø§ÙˆÙ„ÛŒÙ† Ø­Ø§ÙØ¸Ù‡ Ù…ÙˆØ¬ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…
        void* ptr = *it;
        pool.erase(it);
        return ptr;
    }
    
    // Ø§Ú¯Ø± Ø­Ø§ÙØ¸Ù‡â€ŒØ§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø¬Ø¯ÛŒØ¯ ØªØ®ØµÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
    return allocate(size, tag);
}

void MemoryManager::returnPreallocated(void* ptr, const std::string& tag) {
    std::lock_guard<std::mutex> lock(pool_mutex);
    memory_pool[tag].push_back(ptr);
}

void MemoryManager::printMemoryUsage() {
    size_t free, total;
    cudaError_t err = cudaMemGetInfo(&free, &total);
    if (err == cudaSuccess) {
        std::cout << "ğŸ“Š GPU Memory: " 
                  << (total - free) / (1024*1024) << " MB used, " 
                  << free / (1024*1024) << " MB free, " 
                  << total / (1024*1024) << " MB total" << std::endl;
    }
    
    std::cout << "ğŸ“Š Memory pools:" << std::endl;
    for (const auto& pair : memory_pool) {
        std::cout << "  " << pair.first << ": " << pair.second.size() << " blocks" << std::endl;
    }
}

void MemoryManager::clearCache() {
    std::lock_guard<std::mutex> lock(pool_mutex);
    for (auto& pair : memory_pool) {
        for (void* ptr : pair.second) {
            cudaFree(ptr);
        }
        pair.second.clear();
    }
    memory_pool.clear();
}